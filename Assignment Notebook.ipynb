{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment RL Flappy Bird, Alexandre SELVESTREL\n",
    "\n",
    "Make the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import text_flappy_bird_gym\n",
    "import pickle\n",
    "import wandb\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import flappy_bird_gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy version of flappy bird (the one specific to the assignment):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how to process the date for the full screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(obs,n,k,g):# obs is 20x15 initially\n",
    "    line_pnj = round(n/3) - 1\n",
    "    new_pos = 0\n",
    "    found = False\n",
    "    for j in range(k):\n",
    "        if obs[line_pnj,j] == 1:\n",
    "            new_pos = j\n",
    "            found = True\n",
    "    if found is False:\n",
    "        raise ValueError(\"bird not found\")\n",
    "    obstacles_bas = []\n",
    "    i = line_pnj +1\n",
    "    while (len(obstacles_bas) <2) and i<n:\n",
    "            found = False\n",
    "            for j in range(g + 1):\n",
    "                 if (obs[i,j] == 2) and (found is False):\n",
    "                    obstacles_bas.append(i)\n",
    "                    found = True\n",
    "            i += 1\n",
    "    if len(obstacles_bas) == 0:\n",
    "        raise ValueError(\"no obstacle found\")\n",
    "    dist_vert = [bas - line_pnj for bas in obstacles_bas]\n",
    "    gauche = []\n",
    "    droite = []\n",
    "    for i in obstacles_bas:\n",
    "        line = obs[i,:]\n",
    "        found  = False\n",
    "        for j,elem in enumerate(line):\n",
    "            if (elem == 0) and (found is False):\n",
    "                gauche.append(j)\n",
    "                found = True\n",
    "            elif (elem == 2) and (found is True):\n",
    "                droite.append(j - 1)\n",
    "                found = False\n",
    "            elif line[k - 1] == 0:\n",
    "                droite.append(k - 1)\n",
    "                found = False\n",
    "    for a in range(len(obstacles_bas)):\n",
    "        if droite[a] - gauche[a] != g-1:\n",
    "            print(obs)\n",
    "            print('taille erreur',gauche[a],droite[a])\n",
    "            raise ValueError(\"gap mauvaise taille\")\n",
    "    center = [gauche[i] + round(g/2) for i in range(len(droite))]\n",
    "    state = tuple([(new_pos,dist_vert[i],center[i] - new_pos) for i in range(len(dist_vert))])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC:\n",
    "    def __init__(self,eps,alpha,n_max_step,height,width,gap,trained = False, processed = True) -> None:\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.n_max_step = n_max_step\n",
    "        self.nsteps = 0\n",
    "        self.processed = processed\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.gap = gap\n",
    "        if trained is False:\n",
    "            self.q = {}\n",
    "        else:\n",
    "            if self.processed is True:\n",
    "                with open('data/MC.pkl', 'rb') as f:\n",
    "                    self.q = pickle.load(f)\n",
    "            else:\n",
    "                with open('data/MC_non_processed.pkl', 'rb') as f:\n",
    "                    self.q = pickle.load(f)\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode = []\n",
    "        self.nsteps = 0\n",
    "\n",
    "    def chose_action(self,observation, training = True):\n",
    "        if self.processed is True:\n",
    "            state = observation\n",
    "            #print(state)\n",
    "        else:\n",
    "            state = process(observation,n = self.width, k = self.height, g = self.gap)\n",
    "            #print('state',state)\n",
    "            #print(observation)\n",
    "            #time.sleep(0.5)\n",
    "        if (state,0) not in list(self.q.keys()):\n",
    "            self.q[(state,0)] = np.random.uniform(-10,10)\n",
    "        if (state,1) not in list(self.q.keys()):\n",
    "            self.q[(state,1)] = np.random.uniform(-10,10)\n",
    "        if training:\n",
    "            if np.random.uniform(0,1) < self.eps:\n",
    "                action = np.random.choice([0,1])\n",
    "                self.episode.append({'state':state,'action':action})\n",
    "                self.nsteps += 1\n",
    "                return action\n",
    "        action = 0 if self.q[(state,0)] > self.q[(state,1)] else 1\n",
    "        self.episode.append({'state':state,'action':action})\n",
    "        self.nsteps += 1\n",
    "        return action\n",
    "    \n",
    "    def update(self,reward,next_observation,done):\n",
    "        if self.nsteps == self.n_max_step:\n",
    "            reward = 100\n",
    "        if done:\n",
    "            reward = -10\n",
    "        self.episode[-1]['reward'] = reward\n",
    "        if (done is False) and (self.nsteps < self.n_max_step):\n",
    "            pass\n",
    "        else:\n",
    "            G = 0\n",
    "            for j in range(len(self.episode)):\n",
    "                i = len(self.episode) - j - 1\n",
    "                G += self.episode[i]['reward']\n",
    "                self.q[(self.episode[i]['state'],self.episode[i]['action'])] += self.alpha*(G - self.q[(self.episode[i]['state'],self.episode[i]['action'])])\n",
    "    \n",
    "    def save(self):\n",
    "        if self.processed is True:\n",
    "            with open('data/MC.pkl', 'wb') as f:\n",
    "                pickle.dump(self.q, f)\n",
    "        else:\n",
    "            with open('data/MC_non_processed.pkl', 'wb') as f:\n",
    "                pickle.dump(self.q, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa:\n",
    "    def __init__(self,eps,alpha,n_max_step,height,width,gap,trained = False, processed = True, lambd = 0.9) -> None:\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.n_max_step = n_max_step\n",
    "        self.nsteps = 0\n",
    "        self.processed = processed\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.gap = gap\n",
    "        self.lambd = lambd\n",
    "        if trained is False:\n",
    "            self.q = {}\n",
    "            self.e = {}\n",
    "        else:\n",
    "            if self.processed is True:\n",
    "                with open('data/Sarsa_q.pkl', 'rb') as f:\n",
    "                    self.q = pickle.load(f)\n",
    "                with open('data/Sarsa_e.pkl', 'rb') as f:\n",
    "                    self.e = pickle.load(f)\n",
    "            else:\n",
    "                with open('data/Sarsa_non_processed_q.pkl', 'rb') as f:\n",
    "                    self.q = pickle.load(f)\n",
    "                with open('data/Sarsa_non_processed_e.pkl', 'rb') as f:\n",
    "                    self.e = pickle.load(f)\n",
    "\n",
    "    def reset(self):\n",
    "        self.nsteps = 0\n",
    "\n",
    "    def chose_action(self,observation, training = True):\n",
    "        if self.processed is True:\n",
    "            state = observation\n",
    "        else:\n",
    "            state = process(observation,n = self.width, k = self.height, g = self.gap)\n",
    "        self.current_state = state\n",
    "        if (state,0) not in list(self.q.keys()):\n",
    "            self.q[(state,0)] = np.random.uniform(-10,10)\n",
    "            self.e[(state,0)] = 0\n",
    "        if (state,1) not in list(self.q.keys()):\n",
    "            self.q[(state,1)] = np.random.uniform(-10,10)\n",
    "            self.e[(state,1)] = 0\n",
    "        if training:\n",
    "            if np.random.uniform(0,1) < self.eps:\n",
    "                action = np.random.choice([0,1])\n",
    "                self.nsteps += 1\n",
    "                return action\n",
    "        action = 0 if self.q[(state,0)] > self.q[(state,1)] else 1\n",
    "        self.nsteps += 1\n",
    "        self.current_action = action   \n",
    "        return action\n",
    "    \n",
    "    def update(self,reward,next_observation,done):\n",
    "        if self.processed is True or done:\n",
    "            pass\n",
    "        else:\n",
    "            next_observation = process(next_observation,n = self.width, k = self.height, g = self.gap)\n",
    "        if (self.nsteps == self.n_max_step) or done:\n",
    "            max_q = 0\n",
    "        else:\n",
    "            if (next_observation,0) not in list(self.q.keys()):\n",
    "                self.q[(next_observation,0)] = np.random.uniform(-10,10)\n",
    "                self.e[(next_observation,0)] = 0\n",
    "            if (next_observation,1) not in list(self.q.keys()):\n",
    "                self.q[(next_observation,1)] = np.random.uniform(-10,10)\n",
    "                self.e[(next_observation,1)] = 0\n",
    "            max_q = max(self.q[(next_observation,0)],self.q[(next_observation,1)])\n",
    "        delta = reward + max_q - self.q[(self.current_state,self.current_action)]\n",
    "        self.e[(self.current_state,self.current_action)] += 1\n",
    "        for key in self.q.keys():\n",
    "            self.q[key] += self.alpha*delta*self.e[key]\n",
    "            self.e[key] *= self.lambd\n",
    "\n",
    "    def save(self):\n",
    "        if self.processed is True:\n",
    "            with open('data/Sarsa_q.pkl', 'wb') as f:\n",
    "                pickle.dump(self.q, f)\n",
    "            with open('data/Sarsa_e.pkl', 'wb') as f:\n",
    "                pickle.dump(self.e, f)\n",
    "        else:\n",
    "            with open('data/Sarsa_non_processed_q.pkl', 'wb') as f:\n",
    "                pickle.dump(self.q, f)\n",
    "            with open('data/Sarsa_non_processed_e.pkl', 'wb') as f:\n",
    "                pickle.dump(self.e, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\33771\\Documents\\RL\\DM\\text-flappy-bird-gym\\Assignment\\wandb\\run-20240412_222543-fqj7ym8e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL/runs/fqj7ym8e' target=\"_blank\">Sarsa processed= True height: 15 width: 20 gap: 4date: 12th04mo_22h25min43s</a></strong> to <a href='https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL' target=\"_blank\">https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL/runs/fqj7ym8e' target=\"_blank\">https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL/runs/fqj7ym8e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:01<00:00, 81.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score moyen 85.3 nombre reussite totale 0\n",
      "score moyen 85.11 nombre reussite totale 0\n",
      "score moyen 72.55 nombre reussite totale 0\n",
      "score moyen 78.88 nombre reussite totale 0\n",
      "score moyen 79.14 nombre reussite totale 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb93732bc6148d9ae450f67c55c7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>▁▁▁▁▂▂▂▁▄▁▄▂▂▃▂▃▅▄▂▃▁▂▁▂▂▃▂▂▅█▆▄▃█▅▂▇▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>score</td><td>33.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Sarsa processed= True height: 15 width: 20 gap: 4date: 12th04mo_22h25min43s</strong> at: <a href='https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL/runs/fqj7ym8e' target=\"_blank\">https://wandb.ai/alexandre-selvestrel-team/FlappyBird-RL/runs/fqj7ym8e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240412_222543-fqj7ym8e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def training(env, policy,n_episodes, n_max_step,n_testing_moments = 100):\n",
    "    li_scores = []\n",
    "    for num_ep in tqdm(range(n_episodes)):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        policy.reset()\n",
    "        sep_test = n_episodes // n_testing_moments\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        while (done is False) and (n_step < n_max_step):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            action = policy.chose_action(obs,training = True)\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            policy.update(reward,obs,done)\n",
    "        #print(sep_test)\n",
    "        if num_ep % sep_test == 0:\n",
    "            score_moyen, n_terminated = testing(env,policy,n_episodes=1,n_max_step= n_max_step)\n",
    "            li_scores.append(score_moyen)\n",
    "            #print('score',score_moyen)\n",
    "            if do_wandb:\n",
    "                wandb.log({\"score\": score_moyen}, step=num_ep)\n",
    "    return li_scores\n",
    "                \n",
    "\n",
    "def testing(env, policy,n_episodes, n_max_step):\n",
    "    score_moyen = 0\n",
    "    policy.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        obs,info = env.reset()\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        n_terminated = 0 \n",
    "\n",
    "        while (done is False) and (n_step < n_max_step):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            action = policy.chose_action(obs,training = False)\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            if n_step == n_max_step:\n",
    "                n_terminated += 1\n",
    "        score_moyen += n_step\n",
    "    score_moyen = score_moyen/n_episodes\n",
    "    return score_moyen, n_terminated\n",
    "\n",
    "def show(env, policy,n_episodes):\n",
    "    policy.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        obs, info = env.reset()\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        while (done is False):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            action = policy.chose_action(obs,training = False)\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            os.system(\"clear\")\n",
    "            sys.stdout.write(env.render())\n",
    "            time.sleep(0.1) # FPS\n",
    "            print(obs)\n",
    "            print(n_step)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained = False\n",
    "    continue_training = False\n",
    "    n_max_step = 1000\n",
    "    n_training = 5000\n",
    "    n_testing = 100\n",
    "    n_testing_moments = 100\n",
    "    eps = 0.01\n",
    "    alpha = 0.2\n",
    "    name_env = 'TextFlappyBird-v0' #ne dépend pas de la qualité de l'environnement\n",
    "    #name_env = 'TextFlappyBird-screen-v0' #dépendant de la qualité de l'environnement\n",
    "    #name_policy = 'MC'\n",
    "    name_policy = 'Sarsa'\n",
    "    height = 15\n",
    "    width = 20\n",
    "    gap = 4\n",
    "    do_wandb = True\n",
    "    \n",
    "    if name_env == 'TextFlappyBird-v0':\n",
    "        processed = True\n",
    "    elif name_env == 'TextFlappyBird-screen-v0':\n",
    "        processed = False\n",
    "    else:\n",
    "        raise ValueError(\"This environment doesn't exist!\")\n",
    "    for run in range(1):\n",
    "        date = datetime.datetime.now().strftime('%dth%mmo_%Hh%Mmin%Ss')\n",
    "        #print(date)\n",
    "        name = name_policy + ' processed= ' + str(processed) +' height: '+ str(height) + ' width: '+ str(width) + ' gap: ' +str(gap) +'date: '+ str(date)\n",
    "        #print(name)\n",
    "        if do_wandb:\n",
    "            wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"FlappyBird-RL\",\n",
    "            name=name,\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"processed\": processed ,\n",
    "            \"name_policy\": name_policy,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"gap\": gap,\n",
    "            \"True_flappy\": False,\n",
    "            \"eps\": eps,\n",
    "            \"alpha\": alpha,\n",
    "            })\n",
    "        env = gym.make(name_env, height = height, width = width, pipe_gap = gap)\n",
    "        if name_policy == 'MC':\n",
    "            policy = MC(eps = eps,alpha = alpha, n_max_step = n_max_step,height = height,width = width,gap = gap ,trained = trained, processed = processed)\n",
    "        elif name_policy:\n",
    "            policy = Sarsa(eps = eps,alpha = alpha, n_max_step = n_max_step,height = height,width = width,gap = gap ,trained = trained, processed = processed, lambd = 0.009)\n",
    "        if continue_training or not trained:\n",
    "            li_scores = training(env,policy,n_episodes= n_training,n_max_step= n_max_step,n_testing_moments = n_testing_moments)\n",
    "            file_name = 'data/scores' + name + '.pkl'\n",
    "        for _ in range(5):\n",
    "            score_moyen, n_terminated = testing(env,policy,n_episodes= n_testing,n_max_step= n_max_step)\n",
    "            print(\"score moyen\",score_moyen,\"nombre reussite totale\", n_terminated)\n",
    "        policy.save()\n",
    "        env.close()\n",
    "        if do_wandb:\n",
    "            wandb.finish()\n",
    "    #show(env,policy,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Flappy Bird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a new way to process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_true(array,bin_width, bin_height):\n",
    "    width, height = array\n",
    "    width = round(width*(bin_width//2))\n",
    "    height = round(height*(bin_height//2))\n",
    "    return (width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_True:\n",
    "    def __init__(self,eps,alpha,n_max_step,height,width,gap,trained = False) -> None:\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.n_max_step = n_max_step\n",
    "        self.nsteps = 0\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.gap = gap\n",
    "        if trained is False:\n",
    "            self.q = {}\n",
    "        else:\n",
    "            with open('data/MC_true.pkl', 'rb') as f:\n",
    "                self.q = pickle.load(f)\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode = []\n",
    "        self.nsteps = 0\n",
    "\n",
    "    def chose_action(self,observation, training = True):\n",
    "        #print(observation)\n",
    "        state = process_true(observation,self.width,self.height)\n",
    "        #print(state)\n",
    "        if (state,0) not in list(self.q.keys()):\n",
    "            self.q[(state,0)] = np.random.uniform(-10,10)\n",
    "        if (state,1) not in list(self.q.keys()):\n",
    "            self.q[(state,1)] = np.random.uniform(-10,10)\n",
    "        if training:\n",
    "            if np.random.uniform(0,1) < self.eps:\n",
    "                action = np.random.choice([0,1])\n",
    "                self.episode.append({'state':state,'action':action})\n",
    "                self.nsteps += 1\n",
    "                return action\n",
    "        action = 0 if self.q[(state,0)] > self.q[(state,1)] else 1\n",
    "        self.episode.append({'state':state,'action':action})\n",
    "        self.nsteps += 1\n",
    "        return action\n",
    "    \n",
    "    def update(self,reward,next_observation,done):\n",
    "        #print(3*abs(self.episode[-1]['state'][1])/self.width)\n",
    "        reward = reward - 5*abs(self.episode[-1]['state'][1])/self.width\n",
    "        if self.nsteps == self.n_max_step:\n",
    "            reward = 10000\n",
    "        if done:\n",
    "            reward = -30\n",
    "            #print('boum')\n",
    "        self.episode[-1]['reward'] = reward\n",
    "        if (done is False) and (self.nsteps < self.n_max_step):\n",
    "            pass\n",
    "        else:\n",
    "            G = 0\n",
    "            for j in range(len(self.episode)):\n",
    "                i = len(self.episode) - j - 1\n",
    "                G += self.episode[i]['reward']\n",
    "                self.q[(self.episode[i]['state'],self.episode[i]['action'])] += self.alpha*(G - self.q[(self.episode[i]['state'],self.episode[i]['action'])])\n",
    "        \n",
    "    \n",
    "    def save(self):\n",
    "        #print(self.q)\n",
    "        with open('data/MC_true.pkl', 'wb') as f:\n",
    "            pickle.dump(self.q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66079e2ff9de4b33a8dd36d97f2c3f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888888889738, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\33771\\AppData\\Local\\Temp\\ipykernel_21280\\1102407310.py 88 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m#print(name)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_wandb:\n\u001b[1;32m---> 88\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# set the wandb project where this run will be logged\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFlappyBird-RL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# track hyperparameters and run metadata\u001b[39;49;00m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname_policy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrue_flappy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m policy \u001b[38;5;241m=\u001b[39m MC_True(eps \u001b[38;5;241m=\u001b[39m eps,alpha \u001b[38;5;241m=\u001b[39m alpha, n_max_step \u001b[38;5;241m=\u001b[39m n_max_step,height \u001b[38;5;241m=\u001b[39m height,width \u001b[38;5;241m=\u001b[39m width,gap \u001b[38;5;241m=\u001b[39m gap ,trained \u001b[38;5;241m=\u001b[39m trained)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trained:\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1199\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[0;32m   1198\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m-> 1199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1201\u001b[0m     error_seen \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1176\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1174\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1177\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:756\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    755\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run)\n\u001b[1;32m--> 756\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    762\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[0;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\33771\\.conda\\envs\\transform\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def training_true(env, policy,n_episodes, n_max_step, n_testing_moments = 100):\n",
    "    sep_test = n_episodes // n_testing_moments\n",
    "    for num_ep in tqdm(range(n_episodes)):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        policy.reset()\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        while (done is False) and (n_step < n_max_step):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            #time.sleep(0.2)\n",
    "            action = policy.chose_action(obs,training = True)\n",
    "            #print(obs)\n",
    "            #env.render()\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            policy.update(reward,obs,done)\n",
    "        if num_ep % sep_test == 0:\n",
    "            score_moyen, n_terminated = testing_true(env,policy,n_episodes=1,n_max_step= n_max_step)\n",
    "            if do_wandb:\n",
    "                wandb.log({\"score\": score_moyen}, step=num_ep)\n",
    "\n",
    "def testing_true(env, policy,n_episodes, n_max_step):\n",
    "    score_moyen = 0\n",
    "    policy.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        n_terminated = 0 \n",
    "\n",
    "        while (done is False) and (n_step < n_max_step):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            action = policy.chose_action(obs,training = False)\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            if n_step == n_max_step:\n",
    "                n_terminated += 1\n",
    "        score_moyen += n_step\n",
    "    score_moyen = score_moyen/n_episodes\n",
    "    return score_moyen, n_terminated\n",
    "\n",
    "\n",
    "def show_true(env, policy,n_episodes):\n",
    "    policy.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        n_step = 0\n",
    "        # iterate\n",
    "        while (done is False):\n",
    "            n_step += 1\n",
    "            # Select next action\n",
    "            action = policy.chose_action(obs,training = False)\n",
    "            # Appy action and return new observation of the environment\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            time.sleep(1 / 30)  # FPS\n",
    "            print(obs)\n",
    "            print(n_step)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained = True\n",
    "    continue_training = False\n",
    "    n_max_step = 10000\n",
    "    n_training = 5000\n",
    "    n_testing = 100\n",
    "    eps = 0.01\n",
    "    alpha = 0.5\n",
    "    env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
    "    height = 40\n",
    "    width = 50\n",
    "    gap = 4\n",
    "    do_wandb = True\n",
    "    processed = True\n",
    "\n",
    "\n",
    "    for run in range(5):\n",
    "        date = datetime.datetime.now().strftime('%dth%mmo_%Hh%Mmin%Ss')\n",
    "        #print(date)\n",
    "        name = 'MC processed= ' + str(processed) +' height: '+ str(height) + ' width: '+ str(width) + ' gap: ' +str(gap) +'date: '+ str(date)\n",
    "        #print(name)\n",
    "        if do_wandb:\n",
    "            wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=\"FlappyBird-RL\",\n",
    "            name=name,\n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"processed\": processed ,\n",
    "            \"name_policy\": 'MC',\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"gap\": gap,\n",
    "            \"True_flappy\": True,\n",
    "            \"eps\": eps,\n",
    "            \"alpha\": alpha,\n",
    "            })\n",
    "        policy = MC_True(eps = eps,alpha = alpha, n_max_step = n_max_step,height = height,width = width,gap = gap ,trained = trained)\n",
    "        if continue_training or not trained:\n",
    "            training_true(env,policy,n_episodes= n_training,n_max_step= n_max_step)\n",
    "        for _ in range(1):\n",
    "            score_moyen, n_terminated = testing_true(env,policy,n_episodes= n_testing,n_max_step= n_max_step)\n",
    "            print(\"score moyen\",score_moyen,\"nombre reussite totale\", n_terminated)\n",
    "        # Render the game\n",
    "        if continue_training or not trained:\n",
    "            policy.save()\n",
    "        env.close()\n",
    "        if do_wandb:\n",
    "            wandb.finish()\n",
    "    #show_true(env,policy,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
